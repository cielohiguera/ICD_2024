{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84fba0e6-b155-4a54-9e5a-ec08ddca3ed1",
   "metadata": {},
   "source": [
    "### Notebook Bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06105292-0151-465f-9f69-b77686bc1ea5",
   "metadata": {},
   "source": [
    "###### Alumno (a): Cielo Aholiva Higuera Gutiérrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3d147c40-076f-4435-9164-11966647d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy as spc\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d7096-042a-4cf3-929e-717ca9ec3fbe",
   "metadata": {},
   "source": [
    "##### Datos de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a002d9fc-abd8-40e2-8d53-3d79810c1a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      1  ESAS COSAS Y OTRAS PUEDEN PASAR POR MANTENER A...\n",
      "1      1  28: te amodio, odio a la perra de tu amiga ☺️☺...\n",
      "2      1  @LaDivinaDiva Callate maldita perra. O seguro ...\n",
      "3      1  @MarysabelPuerto Mejor callate cara de puta o ...\n",
      "4      1  @xarita327 @TRIKYHUMOR @yonier2012 @casTa1326 ...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"df_mini_HS.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "192a1000-ac10-4a84-b719-b97cf77f6071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           clean_text\n",
      "0   esas cosas y otras pueden pasar por mantener a...\n",
      "1    te amodio odio a la perra de tu amiga pero tú...\n",
      "2    callate maldita perra o seguro eres un pobre ...\n",
      "3    mejor callate cara de puta o reputa como tu m...\n",
      "4                                        cállate puta\n",
      "5                                        callate puta\n",
      "6   y el inmigrante recibe ayuda del rico estado l...\n",
      "7   de los moros no se puede esperar nada bueno y ...\n",
      "8   por que si a una mujer le pegan un tiro en la ...\n",
      "9   analicemos esto si te pones unos shorts así en...\n",
      "10  see tal vez les recordo como peron protegio a ...\n",
      "11  pietrapierce story purs sangs arabes stars des...\n",
      "12  qué dice este de frivolizar el acoso escolar p...\n",
      "13    retira el permiso a  refugiados que fueron d...\n",
      "14  hoy quiero denunciaaaaaaar a la gente puto gua...\n",
      "15  redomicilie su sociedad offshore en emiratos á...\n",
      "16   basta poned pie en pared a tanta provocación ...\n",
      "17  semana de la juventud torneo futbol   futbol  ...\n",
      "18             callate y metete party de una puta vez\n",
      "19  cuántos inmigrantes creemos que hay y cuántos ...\n"
     ]
    }
   ],
   "source": [
    "def limpiar_texto(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r\"@\\S+\", \"\", texto)  # Eliminar menciones a usuarios\n",
    "    texto = re.sub(r\"http[s]?://\\S+\", \"\", texto)  # Eliminar enlaces\n",
    "    texto = re.sub(r\"#\\S+\", \"\", texto)  # Eliminar hashtags\n",
    "    texto = re.sub(r\"[0-9]\", \"\", texto)  # Eliminar números\n",
    "    texto = re.sub(r\"(\\(.*\\))|(\\[.*\\])\", \"\", texto)  # Eliminar paréntesis y corchetes\n",
    "    texto = re.sub(r\"\\n\", \"\", texto)  # Eliminar caracteres de nueva línea\n",
    "    texto = re.sub(r\"(http[s]?://\\S+)|([\\[\\(].*[\\)\\]])|([#@]\\S+)|\\n\", \"\", texto)  # Eliminar varios patrones\n",
    "    texto = re.sub(r\"(\\.)|(,)\", \"\", texto)  # Eliminar puntos y comas\n",
    "    texto = re.sub(r\"[¡!]\", \"\", texto)  # Eliminar signos de admiración \n",
    "    texto = re.sub(r\"[¿?]\", \"\", texto)  # Eliminar signos de exclamación\n",
    "    texto = re.sub(r\"[/]\", \"\", texto) #Eliminar slash\n",
    "    texto = re.sub(r'[^\\w\\s,]', '', texto) # Eliminar emojis\n",
    "    return texto\n",
    "df['clean_text'] = df['text'].apply(limpiar_texto)\n",
    "print(df[['clean_text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cb2c5-8451-485f-8577-9f8459168901",
   "metadata": {},
   "source": [
    "#### Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "94328f70-a0bb-47f5-89a0-745b8843c923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [esas, cosas, y, otras, pueden, pasar, por, ma...\n",
      "1     [te, amodio, odio, a, la, perra, de, tu, amiga...\n",
      "2     [callate, maldita, perra, o, seguro, eres, un,...\n",
      "3     [mejor, callate, cara, de, puta, o, reputa, co...\n",
      "4                                       [cállate, puta]\n",
      "5                                       [callate, puta]\n",
      "6     [y, el, inmigrante, recibe, ayuda, del, rico, ...\n",
      "7     [de, los, moros, no, se, puede, esperar, nada,...\n",
      "8     [por, que, si, a, una, mujer, le, pegan, un, t...\n",
      "9     [analicemos, esto, si, te, pones, unos, shorts...\n",
      "10    [see, tal, vez, les, recordo, como, peron, pro...\n",
      "11    [pietrapierce, story, purs, sangs, arabes, sta...\n",
      "12    [qué, dice, este, de, frivolizar, el, acoso, e...\n",
      "13    [retira, el, permiso, a, refugiados, que, fuer...\n",
      "14    [hoy, quiero, denunciaaaaaaar, a, la, gente, p...\n",
      "15    [redomicilie, su, sociedad, offshore, en, emir...\n",
      "16    [basta, poned, pie, en, pared, a, tanta, provo...\n",
      "17    [semana, de, la, juventud, torneo, futbol, fut...\n",
      "18      [callate, y, metete, party, de, una, puta, vez]\n",
      "19    [cuántos, inmigrantes, creemos, que, hay, y, c...\n",
      "Name: clean_text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Cielo\n",
      "[nltk_data]     Aholiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "tokens = df['clean_text'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cb7497-2ef9-4102-9e2f-243abb9fa2cd",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a875732b-282d-437c-83a3-4e7f389b1e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
      "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.9 MB 162.5 kB/s eta 0:01:20\n",
      "     --------------------------------------- 0.0/12.9 MB 186.2 kB/s eta 0:01:10\n",
      "     --------------------------------------- 0.0/12.9 MB 217.9 kB/s eta 0:00:59\n",
      "     --------------------------------------- 0.1/12.9 MB 252.2 kB/s eta 0:00:51\n",
      "     --------------------------------------- 0.1/12.9 MB 346.5 kB/s eta 0:00:37\n",
      "      -------------------------------------- 0.2/12.9 MB 593.2 kB/s eta 0:00:22\n",
      "     -- ------------------------------------- 0.6/12.9 MB 1.7 MB/s eta 0:00:08\n",
      "     ----- ---------------------------------- 1.7/12.9 MB 4.2 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.3/12.9 MB 7.2 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.6/12.9 MB 9.2 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 6.0/12.9 MB 11.0 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.2/12.9 MB 12.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.5/12.9 MB 13.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.9 MB 13.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.9/12.9 MB 28.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.6/12.9 MB 29.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.9/12.9 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.9/12.9 MB 23.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from es-core-news-sm==3.7.0) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\documentos\\phyton\\phyton programa\\1 csharp\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.0)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b9aec732-6e14-4463-9408-724ceb253be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['esas', 'cosas', 'y', 'otras', 'pueden', 'pasar', 'por', 'mantener', 'a', 'la', 'inmigracion', 'ilegal', 'en', 'españa', 'y', 'no', 'expulsarlos', 'no', 'la', 'agresión', 'de', 'los', 'manteros', 'a', 'un', 'turista', 'pone', 'a', 'colau', 'en', 'un', 'aprieto', 'vía'], ['te', 'amodio', 'odio', 'a', 'la', 'perra', 'de', 'tu', 'amiga', 'pero', 'tú', 'eres', 'mi', 'vida', 'tijerazo'], ['callate', 'maldita', 'perra', 'o', 'seguro', 'eres', 'un', 'pobre', 'marico', 'detras', 'de', 'un', 'user', 'femenino', 'chavista', 'colaborador'], ['mejor', 'callate', 'cara', 'de', 'puta', 'o', 'reputa', 'como', 'tu', 'madre', 'se', 'nota', 'que', 'te', 'hacen', 'falta', 'estos'], ['cállate', 'puta'], ['callate', 'puta'], ['y', 'el', 'inmigrante', 'recibe', 'ayuda', 'del', 'rico', 'estado', 'ladrón', 'que', 'se', 'olvida', 'de', 'los', 'nacionales', 'mientras', 'nos', 'viola', 'y', 'mata', 'pintada', 'con', 'una', 'verdad', 'a', 'medias'], ['de', 'los', 'moros', 'no', 'se', 'puede', 'esperar', 'nada', 'bueno', 'y', 'esto', 'te', 'lo', 'dicen', 'los', 'propios', 'árabes', 'que', 'también', 'han', 'luchado', 'contra', 'ellos'], ['por', 'que', 'si', 'a', 'una', 'mujer', 'le', 'pegan', 'un', 'tiro', 'en', 'la', 'cabeza', 'dura', 'tres', 'dias', 'en', 'morirse', 'porque', 'a', 'los', 'tres', 'dias', 'la', 'bala', 'encuentra', 'el', 'cerebro'], ['analicemos', 'esto', 'si', 'te', 'pones', 'unos', 'shorts', 'así', 'en', 'la', 'calle', 'qué', 'esperas', 'que', 'te', 'digan', 'acoso', 'o', 'provocación'], ['see', 'tal', 'vez', 'les', 'recordo', 'como', 'peron', 'protegio', 'a', 'eichmann', 'y', 'cientos', 'de', 'nazis', 'o', 'les', 'conto', 'a', 'los', 'arabes', 'el', 'chiste', 'del', 'araba', 'que', 'se', 'lo', 'garcho', 'un', 'camello'], ['pietrapierce', 'story', 'purs', 'sangs', 'arabes', 'stars', 'des', 'festivites', 'a', 'agar', 'el', 'm'], ['qué', 'dice', 'este', 'de', 'frivolizar', 'el', 'acoso', 'escolar', 'por', 'favor', 'queréis', 'dejar', 'de', 'decir', 'semejantes', 'tonterías'], ['retira', 'el', 'permiso', 'a', 'refugiados', 'que', 'fueron', 'de', 'vacaciones', 'a', 'sus', 'países', 'vía'], ['hoy', 'quiero', 'denunciaaaaaaar', 'a', 'la', 'gente', 'puto', 'guarra', 'que', 'huele', 'a', 'sudor', 'y', 'chorizo', 'y', 'se', 'sube', 'al', 'bus', 'dejando', 'a', 'tol', 'mundo', 'ko', 'shame', 'on', 'you'], ['redomicilie', 'su', 'sociedad', 'offshore', 'en', 'emiratos', 'árabes', 'unidos'], ['basta', 'poned', 'pie', 'en', 'pared', 'a', 'tanta', 'provocación', 'y', 'cortad', 'la', 'humillación', 'de', 'estos', 'cuatro', 'hijos', 'de', 'perra'], ['semana', 'de', 'la', 'juventud', 'torneo', 'futbol', 'futbol', 'categoria', 'cadete', 'equipos', 'inscritos', 'los', 'yogurines', 'la', 'elite', 'rayo', 'donbenitense', 'los', 'negratas', 'fuenlabrada', 'minato', 'de', 'kiev', 'los', 'josewifakers', 'voolka'], ['callate', 'y', 'metete', 'party', 'de', 'una', 'puta', 'vez'], ['cuántos', 'inmigrantes', 'creemos', 'que', 'hay', 'y', 'cuántos', 'hay', 'en', 'realidad', 'ciudadanos', 'de', 'un', 'lugar', 'llamado', 'mundo']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Cielo\n",
      "[nltk_data]     Aholiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "palabras_filtradas = [palabra for palabra in tokens if palabra not in spanish_stopwords]\n",
    "print(palabras_filtradas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a6f49-de48-4bb6-a5e6-8f66456d203a",
   "metadata": {},
   "source": [
    "#### Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "7938435c-229c-48e3-b0bf-02447499b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def lematizar(palabras):\n",
    "    lema = nlp(\" \".join(palabras))\n",
    "    return \" \".join([token.lemma_ for token in lema])\n",
    "\n",
    "df['oracion_lematizada'] = df['palabras_filtradas'].apply(lematizar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f5de06-57fc-42c4-b486-6b69c482ee8f",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "2a0d6af2-4268-4e17-9ba3-844bf538d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizador = CountVectorizer()\n",
    "vectores = vectorizador.fit_transform([oracion_lematizada])\n",
    "vocabulario = vectorizador.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8312433-d69e-4e8a-a6ac-111fa3337d19",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "ee0882c1-56a5-4cf1-a1ce-4b05c9a96c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración de entrada: 0     [esas, cosas, y, otras, pueden, pasar, por, ma...\n",
      "1     [te, amodio, odio, a, la, perra, de, tu, amiga...\n",
      "2     [callate, maldita, perra, o, seguro, eres, un,...\n",
      "3     [mejor, callate, cara, de, puta, o, reputa, co...\n",
      "4                                       [cállate, puta]\n",
      "5                                       [callate, puta]\n",
      "6     [y, el, inmigrante, recibe, ayuda, del, rico, ...\n",
      "7     [de, los, moros, no, se, puede, esperar, nada,...\n",
      "8     [por, que, si, a, una, mujer, le, pegan, un, t...\n",
      "9     [analicemos, esto, si, te, pones, unos, shorts...\n",
      "10    [see, tal, vez, les, recordo, como, peron, pro...\n",
      "11    [pietrapierce, story, purs, sangs, arabes, sta...\n",
      "12    [qué, dice, este, de, frivolizar, el, acoso, e...\n",
      "13    [retira, el, permiso, a, refugiados, que, fuer...\n",
      "14    [hoy, quiero, denunciaaaaaaar, a, la, gente, p...\n",
      "15    [redomicilie, su, sociedad, offshore, en, emir...\n",
      "16    [basta, poned, pie, en, pared, a, tanta, provo...\n",
      "17    [semana, de, la, juventud, torneo, futbol, fut...\n",
      "18      [callate, y, metete, party, de, una, puta, vez]\n",
      "19    [cuántos, inmigrantes, creemos, que, hay, y, c...\n",
      "Name: clean_text, dtype: object\n",
      "Oración lematizada: 0     [esas, cosas, y, otras, pueden, pasar, por, ma...\n",
      "1     [te, amodio, odio, a, la, perra, de, tu, amiga...\n",
      "2     [callate, maldita, perra, o, seguro, eres, un,...\n",
      "3     [mejor, callate, cara, de, puta, o, reputa, co...\n",
      "4                                       [cállate, puta]\n",
      "5                                       [callate, puta]\n",
      "6     [y, el, inmigrante, recibe, ayuda, del, rico, ...\n",
      "7     [de, los, moros, no, se, puede, esperar, nada,...\n",
      "8     [por, que, si, a, una, mujer, le, pegan, un, t...\n",
      "9     [analicemos, esto, si, te, pones, unos, shorts...\n",
      "10    [see, tal, vez, les, recordo, como, peron, pro...\n",
      "11    [pietrapierce, story, purs, sangs, arabes, sta...\n",
      "12    [qué, dice, este, de, frivolizar, el, acoso, e...\n",
      "13    [retira, el, permiso, a, refugiados, que, fuer...\n",
      "14    [hoy, quiero, denunciaaaaaaar, a, la, gente, p...\n",
      "15    [redomicilie, su, sociedad, offshore, en, emir...\n",
      "16    [basta, poned, pie, en, pared, a, tanta, provo...\n",
      "17    [semana, de, la, juventud, torneo, futbol, fut...\n",
      "18      [callate, y, metete, party, de, una, puta, vez]\n",
      "19    [cuántos, inmigrantes, creemos, que, hay, y, c...\n",
      "Name: clean_text, dtype: object\n",
      "Vectores Bag of Words: [[0 0 1 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Vocabulario: ['acoso' 'agar' 'agresión' 'al' 'amiga' 'amodio' 'analicer' 'aprieto'\n",
      " 'arab' 'arar' 'así' 'ayuda' 'bala' 'bastar' 'bueno' 'bus' 'cabeza'\n",
      " 'cadete' 'callate' 'calle' 'camello' 'cara' 'categoria' 'cerebro'\n",
      " 'chavista' 'chiste' 'chorizo' 'ciento' 'ciudadano' 'colaborador' 'colau'\n",
      " 'como' 'con' 'contar' 'contra' 'cortad' 'cosa' 'creer' 'cuatro' 'cuánto'\n",
      " 'cállate' 'de' 'decir' 'dejar' 'del' 'denunciaaaaaaar' 'des' 'detras'\n",
      " 'dia' 'donbenitense' 'duro' 'eichmann' 'el' 'elite' 'emirato' 'en'\n",
      " 'encontrar' 'equipo' 'escolar' 'ese' 'españa' 'espera' 'esperar' 'estado'\n",
      " 'este' 'expulsar' 'falta' 'favor' 'femenino' 'festivit' 'frivolizar'\n",
      " 'fuenlabrado' 'futbol' 'garcho' 'gente' 'guarrar' 'haber' 'hacer' 'hijo'\n",
      " 'hoy' 'humillación' 'ilegal' 'inmigracion' 'inmigrante' 'inscrito'\n",
      " 'josewifakers' 'juventud' 'kiev' 'ko' 'ladrón' 'llamado' 'luchar' 'lugar'\n",
      " 'madre' 'maldita' 'mantener' 'mantero' 'marico' 'mata' 'medias' 'mejor'\n",
      " 'metete' 'mi' 'mientras' 'minato' 'morir' 'moro' 'mujer' 'mundo'\n",
      " 'nacional' 'nada' 'nazi' 'negrata' 'no' 'notar' 'odiar' 'offshore' 'oler'\n",
      " 'olvidar' 'on' 'otro' 'pared' 'party' 'pasar' 'país' 'peguir' 'permiso'\n",
      " 'pero' 'peron' 'perra' 'pie' 'pietrapierce' 'pintado' 'pobre' 'poder'\n",
      " 'poned' 'poner' 'por' 'porque' 'propio' 'protegio' 'provocación' 'purs'\n",
      " 'puta' 'puto' 'que' 'querer' 'queréis' 'qué' 'rayo' 'realidad' 'recibir'\n",
      " 'recordar' 'redomicilie' 'refugiado' 'reputa' 'retirar' 'rico' 'sang'\n",
      " 'see' 'seguro' 'semana' 'semejante' 'ser' 'shame' 'shorts' 'si'\n",
      " 'sociedad' 'stars' 'story' 'su' 'subir' 'sudor' 'tal' 'también' 'tanto'\n",
      " 'tijerazo' 'tiro' 'tol' 'tontería' 'torneo' 'tres' 'tu' 'turista' 'tú'\n",
      " 'unido' 'uno' 'user' 'vacación' 'verdad' 'vez' 'vida' 'violar' 'voolka'\n",
      " 'vía' 'vío' 'yo' 'yogurín' 'you' 'árabe' 'él']\n"
     ]
    }
   ],
   "source": [
    "print(\"Oración de entrada:\", tokens)\n",
    "print(\"Oración lematizada:\", tokens)\n",
    "print(\"Vectores Bag of Words:\", vectores.toarray())\n",
    "print(\"Vocabulario:\", vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "940065f7-59fa-4f83-827a-8b2bda49fbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   acoso  agar  agresión  al  amiga  amodiir  analicer  aprieto  arab  arar  \\\n",
      "0      0     0         1   0      0        0         0        1     0     0   \n",
      "1      0     0         0   0      1        1         0        0     0     0   \n",
      "2      0     0         0   0      0        0         0        0     0     0   \n",
      "3      0     0         0   0      0        0         0        0     0     0   \n",
      "4      0     0         0   0      0        0         0        0     0     0   \n",
      "\n",
      "   ...  vida  violar  voolka  vía  yo  yogurín  you  árabe  él  label  \n",
      "0  ...     0       0       0    1   0        0    0      0   1      1  \n",
      "1  ...     1       0       0    0   0        0    0      0   0      1  \n",
      "2  ...     0       0       0    0   0        0    0      0   0      1  \n",
      "3  ...     0       0       0    0   0        0    0      0   1      1  \n",
      "4  ...     0       0       0    0   0        0    0      0   0      1  \n",
      "\n",
      "[5 rows x 201 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectores = vectorizer.fit_transform(df['text_limpio'])\n",
    "df_bw = pd.DataFrame(vectores.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_bw['label'] = df['label']  \n",
    "print(df_bw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "937804cb-979a-4e6c-ac6d-cdd24f2f1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bw.to_csv(\"bag_of_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "51308a32-1cd0-4fee-99f0-74091350f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el         25\n",
      "de         16\n",
      "él         13\n",
      "uno        11\n",
      "que         9\n",
      "en          8\n",
      "tú          6\n",
      "este        5\n",
      "decir       4\n",
      "callate     4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Frecuencia de cada palabra\n",
    "word_frequencies = df_bw.drop(columns='label').sum().sort_values(ascending=False)\n",
    "print(word_frequencies.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
